seed: 42
steps: 50000
warmup_steps: 2000
batch_size: 1024
test_batch_size: 32
nhead: 8
d_model: 1024
num_encoder_layers: 6
num_decoder_layers: 6
use_pe: True
use_ls: True
norm_first: False
vocab_size: [3922,3775]
dropout: 0.1
lr: 1e-4
betas: [0.9,0.98]
warmup_init_lr: 1e-7
warmup_end_lr: 1e-4
pad_idx: 0
dataset_type: origin  # 'combined', 'seperate', 'origin'
device: cuda:0
optimizer: Adam

train_path: [
    './dataset/tokenized_dataset_old/train_en.txt', './dataset/tokenized_dataset_old/train_cn.txt']
test_path: [
    './dataset/tokenized_dataset_old/test_en.txt', './dataset/tokenized_dataset_old/test_cn.txt']
val_path: [
    './dataset/tokenized_dataset_old/val_en.txt', './dataset/tokenized_dataset_old/val_cn.txt']

cn_model_path: None
en_model_path: None

output_path: ./results/result_4

log_interval: 100
val_interval: 500
test_interval: 500
accumulate_interval: 1